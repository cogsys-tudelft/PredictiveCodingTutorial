{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.extend([\"./models\"])\n",
    "\n",
    "from pc_model import PCNET, train_model, test_model\n",
    "from mlp import Autoencoder\n",
    "from cnn import CNNAutoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 333\n",
    "\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "    os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loaders(dataset: str):\n",
    "\n",
    "    transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    dataset_path = os.path.abspath(f'../datasets/{dataset}/dataset')\n",
    "    os.makedirs(dataset_path, exist_ok=True)\n",
    "    batch_size = 250\n",
    "\n",
    "    if \"mnist\" == dataset:\n",
    "        DatasetClass = torchvision.datasets.MNIST\n",
    "    elif \"fmnist\" == dataset:\n",
    "        DatasetClass = torchvision.datasets.FashionMNIST\n",
    "    elif \"cifar10\" == dataset:\n",
    "        DatasetClass = torchvision.datasets.CIFAR10\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported dataset.\")\n",
    "\n",
    "    train_dataset = DatasetClass(root=dataset_path, train=True, transform=transform, download=True)\n",
    "    test_dataset  = DatasetClass(root=dataset_path, train=False, transform=transform, download=True)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_reconstruction(model, inputs, is_pc):\n",
    "    original = inputs[0].cpu()\n",
    "    a = model.module_dict['variational'][0] if is_pc else inputs\n",
    "    reconstruction = model.forward(a)[0].detach().cpu()\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(6, 3))\n",
    "\n",
    "    if original.shape[0] == 3:\n",
    "        axs[0].imshow(original.permute(1, 2, 0).numpy())\n",
    "        axs[1].imshow(reconstruction.permute(1, 2, 0).numpy())\n",
    "    else:\n",
    "        axs[0].imshow(original.squeeze().numpy().reshape(28, 28), cmap='gray')\n",
    "        axs[1].imshow(reconstruction.numpy().reshape(28, 28), cmap='gray')\n",
    "\n",
    "    axs[0].set_title('Original')\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    axs[1].set_title('Reconstruction')\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(model, is_pc, train_loader, test_loader, epochs, optimizer, criterion, inference_lr, inference_iterations, inference_momentum, plot_reconstruction):\n",
    "    device = next(model.parameters()).device\n",
    "    best_loss = 10**10\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        with tqdm(total=len(train_loader), desc=f\"Epoch {epoch + 1}/{epochs}\", leave=False) as progress_bar:\n",
    "            for train_inputs, labels in train_loader:\n",
    "                \n",
    "                train_inputs = train_inputs.view(-1, 28 * 28).to(device) if train_inputs.shape[1:] == (1, 28, 28) else train_inputs.to(device)\n",
    "                loss = train_model(\n",
    "                    model,\n",
    "                    train_inputs,\n",
    "                    optimizer,\n",
    "                    criterion,\n",
    "                    inference_lr,\n",
    "                    inference_iterations,\n",
    "                    inference_momentum,\n",
    "                    pc_mode=is_pc\n",
    "                )\n",
    "                progress_bar.update(1)\n",
    "\n",
    "            if plot_reconstruction:\n",
    "                show_reconstruction(model, train_inputs, is_pc)\n",
    "            \n",
    "            model.eval()\n",
    "            losses = []\n",
    "            \n",
    "            for test_inputs, labels in test_loader:\n",
    "                test_inputs = test_inputs.view(-1, 28 * 28).to(device) if test_inputs.shape[1:] == (1, 28, 28) else test_inputs.to(device)\n",
    "\n",
    "                loss = test_model(\n",
    "                    model=model,\n",
    "                    stimuli=test_inputs,\n",
    "                    criterion=criterion,\n",
    "                    inference_lr=inference_lr,\n",
    "                    inference_time=inference_iterations,\n",
    "                    momentum=inference_momentum,\n",
    "                    pc_mode=is_pc\n",
    "                )\n",
    "                losses.append(loss)\n",
    "\n",
    "            if plot_reconstruction:\n",
    "                show_reconstruction(model, test_inputs, is_pc)\n",
    "\n",
    "            best_loss = min(best_loss, sum(losses)/len(losses))\n",
    "            #print(best_loss, sum(losses)/len(losses))\n",
    "            model.train()\n",
    "            \n",
    "    \n",
    "    return best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print('Available device: ', device)\n",
    "\n",
    "epochs = 5\n",
    "trials = 5\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "datasets = ['mnist', 'fmnist', 'cifar10']\n",
    "plot_reconstruction = False\n",
    "\n",
    "inference_lr = 0.175\n",
    "inference_iterations = 35\n",
    "inference_momentum = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    pc_results = []\n",
    "    bp_results = []\n",
    "\n",
    "    train_loader, test_loader = create_loaders(dataset)\n",
    "    print(f'Comparing PC and BP on {dataset} dataset...')\n",
    "\n",
    "    if dataset in ['mnist', 'fashionmnist']:\n",
    "        generative_lr_pc = 0.00035\n",
    "        generative_wd_pc = 0.00002\n",
    "\n",
    "        generative_lr_bp = 0.00035 \n",
    "        generative_wd_bp = 0.00005\n",
    "\n",
    "    elif dataset == 'cifar10':\n",
    "        generative_lr_pc = 0.001\n",
    "        generative_wd_pc = 0.000025\n",
    "\n",
    "        generative_lr_bp = 0.0005\n",
    "        generative_wd_bp = 0.00003\n",
    "\n",
    "    for run in range(trials):\n",
    "\n",
    "        bp_model = CNNAutoencoder(3) if dataset == 'cifar10' else Autoencoder() \n",
    "        bp_optimizer = torch.optim.Adam(bp_model.parameters(), lr=generative_lr_bp, weight_decay=generative_wd_bp)\n",
    "\n",
    "        pc_model = PCNET(bp_model.get_decoder(), batch_size=250)\n",
    "        pc_optimizer = torch.optim.Adam(pc_model.module_dict['generative'].parameters(), lr=generative_lr_pc, weight_decay=generative_wd_pc)\n",
    "\n",
    "        bp_model.to(device)\n",
    "        pc_model.to(device)\n",
    "\n",
    "        start_time = time.time()\n",
    "        pc_best_loss = train_test(pc_model, True, train_loader, test_loader, epochs, pc_optimizer, criterion, inference_lr, inference_iterations, inference_momentum, plot_reconstruction)\n",
    "        pc_training_time = time.time() - start_time\n",
    "        print(f\"Predictive Coding best loss: {pc_best_loss:.5f} - total training time: {pc_training_time:.2f} s\")\n",
    "        pc_results.append({\"run\": run + 1, \"best_loss\": pc_best_loss, \"training_time_sec\": pc_training_time})\n",
    "\n",
    "        start_time = time.time()\n",
    "        bp_best_loss = train_test(bp_model, False, train_loader, test_loader, epochs, bp_optimizer, criterion, inference_lr, inference_iterations, inference_momentum, plot_reconstruction)\n",
    "        bp_training_time = time.time() - start_time\n",
    "        print(f\"Backpropagation best loss: {bp_best_loss:.5f} - total training time: {bp_training_time:.2f} s\")\n",
    "        bp_results.append({\"run\": run + 1, \"best_loss\": bp_best_loss, \"training_time_sec\": bp_training_time})\n",
    "\n",
    "    pc_results = pd.DataFrame(pc_results)\n",
    "    bp_results = pd.DataFrame(bp_results)\n",
    "\n",
    "    os.makedirs(\"./results\", exist_ok=True)\n",
    "    pc_results.to_csv(f\"./results/pc_performance_{dataset}.csv\", index=False)\n",
    "    bp_results.to_csv(f\"./results/bp_performance_{dataset}.csv\", index=False)\n",
    "\n",
    "    print('-'*50)\n",
    "    print(\"Predictive Coding Performance:\")\n",
    "    print(f\"Loss: Mean = {pc_results['best_loss'].mean():.5f}, Std = {pc_results['best_loss'].std():.5f}\")\n",
    "    print(f\"Training Time: Mean = {pc_results['training_time_sec'].mean():.2f} s, Std = {pc_results['training_time_sec'].std():.2f} s\")\n",
    "\n",
    "    print(\"\\nBackpropagation Performance:\")\n",
    "    print(f\"Loss: Mean = {bp_results['best_loss'].mean():.5f}, Std = {bp_results['best_loss'].std():.5f}\")\n",
    "    print(f\"Training Time: Mean = {bp_results['training_time_sec'].mean():.2f} s, Std = {bp_results['training_time_sec'].std():.2f} s\")\n",
    "    print('-'*50)\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in ['mnist', 'fmnist', 'cifar10']:\n",
    "    pc_results = pd.read_csv(f'./results/pc_performance_{dataset}.csv')\n",
    "    bp_results = pd.read_csv(f'./results/bp_performance_{dataset}.csv')\n",
    "\n",
    "    print(f\"\\n--- Results on {dataset.upper()} ---\")\n",
    "\n",
    "    print(\"\\nPredictive Coding:\")\n",
    "    print(f\"Loss:     {pc_results['best_loss'].mean():.4f} ± {pc_results['best_loss'].std():.5f}\")\n",
    "    print(f\"Train time:   {(pc_results['training_time_sec']/5).mean():.2f} ± {(pc_results['training_time_sec']/5).std():.2f} s\")\n",
    "\n",
    "    print(\"\\nBackpropagation:\")\n",
    "    print(f\"Loss:     {bp_results['best_loss'].mean():.4} ± {bp_results['best_loss'].std():.5f}\")\n",
    "    print(f\"Train time:   {(bp_results['training_time_sec']/5).mean():.2f} ± {(bp_results['training_time_sec']/5).std():.2f} s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
